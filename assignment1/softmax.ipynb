{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs231n/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.76078250857\n",
      "9.43823051884\n",
      "8.36699744347\n",
      "9.30796196944\n",
      "9.97420168875\n",
      "11.092643801\n",
      "10.8056156728\n",
      "9.11230508302\n",
      "9.79579698611\n",
      "10.1630975977\n",
      "8.6193718325\n",
      "10.2060016725\n",
      "11.9918922921\n",
      "9.43697824608\n",
      "9.39730044683\n",
      "8.31770014187\n",
      "9.79997141049\n",
      "9.60338436973\n",
      "11.1841627931\n",
      "10.9629080668\n",
      "9.93875157576\n",
      "10.5868761665\n",
      "8.27085985085\n",
      "9.63752975875\n",
      "9.14248206391\n",
      "8.16032855291\n",
      "9.8736335376\n",
      "8.69402485398\n",
      "9.18602206716\n",
      "9.19098988152\n",
      "8.54366233144\n",
      "13.2365863921\n",
      "9.82059800068\n",
      "8.49725233139\n",
      "9.45063861871\n",
      "8.87505548991\n",
      "7.39064187677\n",
      "10.0744644326\n",
      "9.9433094192\n",
      "11.3330148716\n",
      "7.85835725253\n",
      "15.2784195731\n",
      "8.21802559718\n",
      "9.6736396072\n",
      "9.13690781992\n",
      "7.50380642886\n",
      "10.7148303708\n",
      "8.02871111642\n",
      "10.8845396238\n",
      "8.44954633306\n",
      "10.2357920024\n",
      "8.56046855659\n",
      "12.0752017135\n",
      "10.4516390856\n",
      "9.72424474215\n",
      "9.07898479302\n",
      "9.57542027189\n",
      "8.89566009625\n",
      "10.6675745287\n",
      "10.5040396893\n",
      "9.00203536783\n",
      "7.16443503032\n",
      "10.7526553491\n",
      "8.77702157546\n",
      "8.42716159057\n",
      "10.1538471404\n",
      "8.51066212545\n",
      "9.48351999969\n",
      "9.89361413157\n",
      "10.2825066351\n",
      "8.61654573304\n",
      "10.1424634863\n",
      "8.55286319555\n",
      "9.23514380885\n",
      "8.60542239185\n",
      "9.56939782753\n",
      "8.21897083568\n",
      "8.76940169611\n",
      "10.3256522613\n",
      "9.08472990502\n",
      "10.1057136893\n",
      "10.2111136741\n",
      "9.06418397134\n",
      "9.14719104883\n",
      "12.4283006297\n",
      "11.5899275501\n",
      "9.38906021804\n",
      "9.82261035579\n",
      "9.49305897765\n",
      "13.4245287006\n",
      "7.98555961572\n",
      "10.2266711287\n",
      "13.0435256177\n",
      "9.88339727889\n",
      "9.80547274771\n",
      "10.2600546047\n",
      "10.8008229242\n",
      "10.7732536988\n",
      "13.4215420302\n",
      "9.5291119235\n",
      "10.5167571851\n",
      "10.9712470372\n",
      "9.25631354443\n",
      "8.81253816664\n",
      "11.3655636823\n",
      "9.1420214613\n",
      "9.08919601612\n",
      "8.32217892621\n",
      "10.2098226593\n",
      "10.7786556175\n",
      "9.01702974349\n",
      "9.56842705325\n",
      "8.62757960628\n",
      "9.64282485891\n",
      "9.35977684043\n",
      "10.3354409988\n",
      "9.22962838659\n",
      "10.6464989753\n",
      "9.8880051151\n",
      "9.45892352243\n",
      "8.92212746518\n",
      "7.29622465171\n",
      "8.8776483587\n",
      "9.63237578208\n",
      "8.36039521048\n",
      "9.3881187198\n",
      "9.19563475499\n",
      "10.1183833914\n",
      "9.64688898194\n",
      "9.07512923945\n",
      "9.39771438205\n",
      "10.9471314328\n",
      "8.89293788985\n",
      "7.95984687949\n",
      "9.25521685558\n",
      "8.44381607195\n",
      "9.07457339394\n",
      "9.16596034202\n",
      "8.43133202516\n",
      "10.0564202281\n",
      "9.30296697039\n",
      "8.80714227558\n",
      "8.28358518456\n",
      "9.38660219014\n",
      "9.44010188679\n",
      "8.67006182222\n",
      "8.74036537449\n",
      "8.52522762285\n",
      "8.98145019857\n",
      "9.43704847554\n",
      "9.98623580278\n",
      "9.61170945648\n",
      "9.12752178752\n",
      "7.40159955305\n",
      "8.82441737759\n",
      "11.3584102136\n",
      "9.21513426159\n",
      "9.21081025195\n",
      "10.0080159842\n",
      "9.67553087422\n",
      "12.0028019733\n",
      "9.65788230479\n",
      "7.73541597528\n",
      "6.77913863474\n",
      "9.93014334269\n",
      "9.44169854435\n",
      "8.14988771378\n",
      "9.17516999531\n",
      "8.85658598146\n",
      "9.20710825249\n",
      "10.5410737697\n",
      "12.2394411775\n",
      "7.48345602192\n",
      "8.70086928738\n",
      "8.53250369948\n",
      "8.44261155704\n",
      "9.68826684987\n",
      "11.4518904358\n",
      "9.29580674343\n",
      "11.2704144722\n",
      "9.16256925706\n",
      "10.8434358312\n",
      "8.1709049155\n",
      "8.65061907242\n",
      "8.89404328151\n",
      "10.512111512\n",
      "7.35031902802\n",
      "8.85383627265\n",
      "9.31365808436\n",
      "9.49090734279\n",
      "9.96588130685\n",
      "9.0745196676\n",
      "8.94643701649\n",
      "9.01388227508\n",
      "9.93553409608\n",
      "8.79684483563\n",
      "9.54336485022\n",
      "9.28259597593\n",
      "8.80947368736\n",
      "6.07258848911\n",
      "8.37080482293\n",
      "7.87302369175\n",
      "8.96931908117\n",
      "8.77729528331\n",
      "10.4375138638\n",
      "9.13113899757\n",
      "8.42650275068\n",
      "9.50943052355\n",
      "9.38470656186\n",
      "10.5889674141\n",
      "10.025508883\n",
      "10.1619589884\n",
      "12.9510490966\n",
      "10.3841399025\n",
      "9.7820409186\n",
      "10.8600865608\n",
      "9.34626811141\n",
      "9.90087522654\n",
      "10.010498841\n",
      "12.5268892466\n",
      "9.33084746245\n",
      "9.55896851693\n",
      "8.09681528093\n",
      "10.4241861398\n",
      "9.63226588069\n",
      "9.78374458195\n",
      "9.58907532566\n",
      "7.62999299454\n",
      "9.05068609565\n",
      "8.934147174\n",
      "8.31243545404\n",
      "9.2427071603\n",
      "9.1587657023\n",
      "10.4008162429\n",
      "8.3841645348\n",
      "7.29586718727\n",
      "10.4887762304\n",
      "9.27327615303\n",
      "10.4051602873\n",
      "9.26885586289\n",
      "9.70972237288\n",
      "8.76176170208\n",
      "10.7684890095\n",
      "8.78551280917\n",
      "11.0909324002\n",
      "8.2845795977\n",
      "10.6925136372\n",
      "9.58212682249\n",
      "10.5444612345\n",
      "11.3521012376\n",
      "9.74287213558\n",
      "9.49920793827\n",
      "9.98310536264\n",
      "9.60987803051\n",
      "9.89148838934\n",
      "9.42126755847\n",
      "8.83842528873\n",
      "8.65135194398\n",
      "10.5021482931\n",
      "9.12981830322\n",
      "9.88281895629\n",
      "10.448336541\n",
      "9.38912611867\n",
      "10.7181623553\n",
      "7.40856154511\n",
      "8.61996490344\n",
      "8.43356383114\n",
      "8.28773154301\n",
      "8.13959881457\n",
      "8.92291682834\n",
      "10.7035104663\n",
      "9.79949557045\n",
      "8.12125589814\n",
      "10.1946645367\n",
      "9.37785911014\n",
      "9.97785757955\n",
      "8.19587946277\n",
      "9.64978486556\n",
      "7.73371311928\n",
      "8.86432319429\n",
      "10.4165578878\n",
      "9.71028806273\n",
      "7.77881293605\n",
      "11.525400595\n",
      "10.8546958654\n",
      "8.86079045393\n",
      "8.83526183481\n",
      "9.05327446553\n",
      "10.0416098343\n",
      "9.26993719461\n",
      "7.93715272593\n",
      "9.12582199614\n",
      "9.96572670428\n",
      "11.384604231\n",
      "8.43324936656\n",
      "9.24199656091\n",
      "8.15176936045\n",
      "9.82927851298\n",
      "8.50292349072\n",
      "8.46427210543\n",
      "9.68661690816\n",
      "9.35841164474\n",
      "9.21077134253\n",
      "11.1601196984\n",
      "9.06631482797\n",
      "8.68019301592\n",
      "6.87648367379\n",
      "8.83736390139\n",
      "8.97565724043\n",
      "10.1368453187\n",
      "8.44811064333\n",
      "8.76490574567\n",
      "12.1125490351\n",
      "10.2095878105\n",
      "8.41761051998\n",
      "9.73752385742\n",
      "8.53288142952\n",
      "12.6056305787\n",
      "7.99526920197\n",
      "9.0578539912\n",
      "9.45320348539\n",
      "8.80114276442\n",
      "8.45570050034\n",
      "9.31702708645\n",
      "9.90203510079\n",
      "9.19646650562\n",
      "8.138061275\n",
      "9.06027869435\n",
      "8.92801261483\n",
      "9.1294267248\n",
      "9.93210378222\n",
      "8.2235498904\n",
      "9.93355251451\n",
      "9.90568958914\n",
      "9.29089619052\n",
      "8.53822631844\n",
      "9.53197278667\n",
      "10.8806135092\n",
      "9.1466100974\n",
      "8.98049784176\n",
      "9.40052218698\n",
      "9.69237118812\n",
      "9.97222456048\n",
      "10.3039081927\n",
      "10.1832842215\n",
      "10.9649051974\n",
      "9.99752075464\n",
      "10.7331972802\n",
      "8.82375581927\n",
      "9.03412914105\n",
      "9.55797842691\n",
      "8.90211936533\n",
      "8.45012394216\n",
      "8.61476212391\n",
      "8.98683089881\n",
      "9.82316718177\n",
      "8.19031899481\n",
      "8.90283297988\n",
      "8.31474815662\n",
      "9.21761412184\n",
      "8.72030667004\n",
      "11.9404693873\n",
      "8.9571678702\n",
      "9.37793670599\n",
      "9.89484184051\n",
      "8.3325857542\n",
      "7.39201603409\n",
      "10.8124739067\n",
      "7.99350781639\n",
      "8.82888454248\n",
      "10.2882078055\n",
      "9.7386274434\n",
      "10.5788568713\n",
      "11.201850594\n",
      "8.77243202165\n",
      "8.89655205046\n",
      "6.97572073497\n",
      "10.9439112022\n",
      "11.5151585452\n",
      "9.38181914756\n",
      "8.82540570596\n",
      "8.62329352635\n",
      "11.8553537342\n",
      "9.77632864212\n",
      "8.76374345626\n",
      "9.40668328035\n",
      "8.47267003381\n",
      "8.78323590404\n",
      "11.3509625024\n",
      "10.8527341208\n",
      "14.5521472566\n",
      "9.09038903565\n",
      "9.96225191469\n",
      "9.12350202282\n",
      "9.21539340135\n",
      "8.85970560725\n",
      "8.58906891401\n",
      "9.26145202236\n",
      "9.23201645041\n",
      "10.5131734836\n",
      "8.77489657071\n",
      "9.5536119666\n",
      "8.47251850339\n",
      "9.69914652651\n",
      "10.2514131199\n",
      "8.17400975604\n",
      "9.87931090646\n",
      "10.3238461047\n",
      "8.7020534545\n",
      "10.3994339253\n",
      "10.4726825719\n",
      "8.64357802043\n",
      "9.00754875226\n",
      "11.9381665638\n",
      "9.88538387848\n",
      "8.84295539661\n",
      "9.42226764935\n",
      "9.62744582181\n",
      "8.12770537673\n",
      "8.89874256176\n",
      "8.52285266847\n",
      "10.3428054219\n",
      "10.8699168562\n",
      "7.74819577763\n",
      "7.7845086171\n",
      "8.46265345272\n",
      "9.72625216809\n",
      "8.70141227668\n",
      "9.0283889382\n",
      "10.744643064\n",
      "8.58489162117\n",
      "7.0045613189\n",
      "8.6945247179\n",
      "8.58643647079\n",
      "9.22582579594\n",
      "9.75069262454\n",
      "9.23179957634\n",
      "12.5127506683\n",
      "8.29573989336\n",
      "9.58024033462\n",
      "10.2648897481\n",
      "9.65298732885\n",
      "9.92776833634\n",
      "8.9351568149\n",
      "11.3579403816\n",
      "8.5169430669\n",
      "9.31736600187\n",
      "8.79497195108\n",
      "10.0412307155\n",
      "9.58214715961\n",
      "8.89369769663\n",
      "9.88905647715\n",
      "8.38849582544\n",
      "8.80426513165\n",
      "11.6105182466\n",
      "8.85624360267\n",
      "9.97688796046\n",
      "9.97304280889\n",
      "8.84488837315\n",
      "6.83218714101\n",
      "9.86964766758\n",
      "7.84107410958\n",
      "9.58437650232\n",
      "8.06309383841\n",
      "8.63390196688\n",
      "7.71755748738\n",
      "10.2248816446\n",
      "12.346133707\n",
      "9.80690237437\n",
      "9.61930496783\n",
      "9.92023279896\n",
      "10.4234116969\n",
      "9.95409552456\n",
      "8.07031708665\n",
      "9.78825529212\n",
      "9.93703351798\n",
      "10.5929161487\n",
      "8.14394217964\n",
      "9.33084768864\n",
      "9.97004906212\n",
      "9.7611341919\n",
      "10.2287167759\n",
      "9.10064389241\n",
      "11.4712688558\n",
      "9.55238884467\n",
      "7.91737586899\n",
      "10.4700934449\n",
      "9.13495956312\n",
      "10.1727060097\n",
      "8.78888802428\n",
      "9.43456635399\n",
      "9.09325131737\n",
      "10.0946752742\n",
      "9.90808049156\n",
      "8.87511563225\n",
      "8.89953066212\n",
      "9.38664961964\n",
      "8.93752560028\n",
      "8.91545143083\n",
      "8.02412759827\n",
      "(500, 10)\n",
      "()\n",
      "(500, 3073)\n",
      "(3073, 10)\n",
      "loss: 1136.300744\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
